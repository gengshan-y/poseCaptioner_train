{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import caffe\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxWords = 100\n",
    "# dataPath = '/data/gengshan/pose_s2vt/hdf5/buffer_32_s2vt_100/small_train_batches/batch_0.h5'\n",
    "dataPath = '/data/gengshan/pose_s2vt/hdf5/buffer_32_s2vt_100/train_2_batches/batch_0.h5'          # change h5 data path\n",
    "LSTM_NET_FILE = './captioner.prototxt'                                                            # change net \n",
    "# MODEL_FILE = '/data/gengshan/pose_s2vt/snapshots/stored/s2vt_asl_pose_iter_11668.caffemodel'\n",
    "MODEL_FILE = '/data/gengshan/pose_s2vt/snapshots/s2vt_asl_pose_iter_50000.caffemodel'             # change model path\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n",
    "lstm_net = caffe.Net(LSTM_NET_FILE, MODEL_FILE, caffe.TEST)\n",
    "vocabFile = '/data/gengshan/pose_s2vt/whole_vocabulary.txt'                                           # change vocab\n",
    "UNK_IDENTIFIER = '<en_unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_inds_to_sentence(vocab, inds):\n",
    "    sentence = ' '.join([vocab[i] for i in inds])\n",
    "    # Capitalize first character.\n",
    "    sentence = sentence[0].upper() + sentence[1:]\n",
    "    # Replace <EOS> with '.', or append '...'.\n",
    "    if sentence.endswith(' ' + vocab[0]):\n",
    "        sentence = sentence[:-(len(vocab[0]) + 1)] + '.'\n",
    "    else:\n",
    "        sentence += '...'\n",
    "    return sentence\n",
    "\n",
    "def init_vocab_from_file(vocabFilePath):\n",
    "    # initialize the vocabulary with the UNK word\n",
    "    vocabulary = {UNK_IDENTIFIER: 0}\n",
    "    vocabulary_inverted = [UNK_IDENTIFIER]\n",
    "    num_words_dataset = 0\n",
    "    with open(vocabFilePath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            # print word\n",
    "            if word == UNK_IDENTIFIER:\n",
    "                continue\n",
    "            else:\n",
    "                assert word not in vocabulary\n",
    "            num_words_dataset += 1\n",
    "            vocabulary[word] = len(vocabulary_inverted)\n",
    "            vocabulary_inverted.append(word)\n",
    "    num_words_vocab = len(vocabulary.keys())\n",
    "    print ('Initialized vocabulary from file with %d unique words ' +\n",
    "       '(from %d total words in dataset).') % \\\n",
    "      (num_words_vocab, num_words_dataset)\n",
    "    assert len(vocabulary_inverted) == num_words_vocab\n",
    "    return vocabulary_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized vocabulary from file with 13852 unique words (from 13851 total words in dataset).\n"
     ]
    }
   ],
   "source": [
    "vocabList =  ['<EOS>'] + init_vocab_from_file(vocabFile)\n",
    "\n",
    "wholeData = h5py.File(dataPath, 'r')\n",
    "poseData = []\n",
    "targetSent = []\n",
    "existIdict = []\n",
    "stageIdict = []\n",
    "inputSent = []\n",
    "\n",
    "streamSize,batchSize,_ = wholeData['frame_fc7'].shape\n",
    "for streamIdx in range(0, streamSize/maxWords):\n",
    "    for batchIdx in range(0, batchSize):\n",
    "        poseData.append(wholeData['frame_fc7'][streamIdx * maxWords: (streamIdx + 1) * maxWords, batchIdx: batchIdx + 1, :])\n",
    "        inputSent.append(wholeData['input_sentence'][streamIdx * maxWords: (streamIdx + 1) * maxWords, batchIdx: batchIdx + 1])\n",
    "        existIdict.append(wholeData['cont_sentence'][streamIdx * maxWords: (streamIdx + 1) * maxWords, batchIdx: batchIdx + 1])\n",
    "        stageIdict.append(wholeData['stage_indicator'][streamIdx * maxWords: (streamIdx + 1) * maxWords, batchIdx: batchIdx + 1])\n",
    "        targetSent.append(wholeData['target_sentence'][streamIdx * maxWords: (streamIdx + 1) * maxWords, batchIdx: batchIdx + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref = open('train_log/ref.txt', 'w')\n",
    "res = open('train_log/res.txt', 'w')\n",
    "for it in range(0, len(inputSent)):\n",
    "    probs = lstm_net.forward(frames_fc7=poseData[it], cont_sentence=existIdict[it], input_sentence=inputSent[it],\\\n",
    "                             stage_indicator=stageIdict[it])['probs']\n",
    "    \n",
    "    predictIdx = np.squeeze(stageIdict[it] * np.argmax(probs, axis=2))\n",
    "\n",
    "    predictIdx = predictIdx[np.where(stageIdict[it] != 0)[0][0]:]\n",
    "    if 0 in predictIdx:\n",
    "        predictIdx = predictIdx[:np.where(predictIdx == 0)[0][0]]\n",
    "    \n",
    "    res.write(vocab_inds_to_sentence(vocabList, [int(x) for x in predictIdx]) + '\\n')\n",
    "    ref.write(vocab_inds_to_sentence(vocabList, [int(x) for x in targetSent[it] if x != -1]) + '\\n')\n",
    "ref.close()\n",
    "res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
