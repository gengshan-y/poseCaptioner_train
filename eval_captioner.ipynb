{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "from dataLoader import fc7FrameSequenceGenerator\n",
    "from hdf5_npstreamsequence_generator import HDF5SequenceWriter\n",
    "from fileWriter import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 100\n",
    "BUFFER_SIZE = 1 # TEXT streams\n",
    "NUM_OUT_PER_CHUNK = 20\n",
    "START_CHUNK = 0\n",
    "\n",
    "SETTING = '/data/gengshan/pose_s2vt'\n",
    "FRAMEFEAT_FILE_PATTERN = SETTING + '/splits/dataCsv_{0}.txt'\n",
    "SENTS_FILE_PATTERN = SETTING + '/splits/dataTsv_{0}.txt'  # input paths\n",
    "vocab_file = '/data/gengshan/s2vt/vocabulary_whole.txt'\n",
    "\n",
    "LSTM_NET_FILE = './s2vt.words_to_preds.deploy.prototxt'\n",
    "RESULTS_DIR = SETTING + '/results'\n",
    "NET_TAG = 's2vt_asl_pose_iter_11668'\n",
    "# NET_TAG = 's2vt_asl_pose_iter_40809'\n",
    "MODEL_FILE = SETTING + '/snapshots/stored/' + NET_TAG +'.caffemodel'\n",
    "# MODEL_FILE = SETTING + '/snapshots/' + NET_TAG +'.caffemodel'\n",
    "STRATEGIES = [{'type': 'beam', 'beam_size': 1}]\n",
    "\n",
    "data_split_name = 'haha'   # sv files\n",
    "\n",
    "filenames =  [FRAMEFEAT_FILE_PATTERN.format(data_split_name),\n",
    "               SENTS_FILE_PATTERN.format(data_split_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading frame features from file: /data/gengshan/pose_s2vt/splits/dataCsv_haha.txt\n",
      "Reading sentences in: /data/gengshan/pose_s2vt/splits/dataTsv_haha.txt\n",
      "Initializing the vocabulary.\n",
      "Initialized vocabulary from file with 6067 unique words (from 6066 total words in dataset).\n"
     ]
    }
   ],
   "source": [
    "fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE,\n",
    "      vocab_file, max_words=MAX_WORDS, align=False, shuffle=False,\n",
    "      pad=False, truncate=False, reverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/600 (0.000000%) lines\n",
      "Processed 1/600 (0.166667%) lines\n",
      "Processed 100/600 (16.666667%) lines\n",
      "Processed 200/600 (33.333333%) lines\n",
      "Processed 300/600 (50.000000%) lines\n",
      "Processed 400/600 (66.666667%) lines\n",
      "Processed 500/600 (83.333333%) lines\n",
      "Processed 0/600 (0.000000%) lines\n",
      "Read 600 videos pool feats\n"
     ]
    }
   ],
   "source": [
    "video_gt_pairs = all_video_gt_pairs(fsg)  # target sentence\n",
    "print 'Read %d videos pool feats' % len(fsg.vid_framefeats)\n",
    "NUM_CHUNKS = 2  # (len(fsg.vid_framefeats)/NUM_OUT_PER_CHUNK) + 1\n",
    "# add english inverted vocab \n",
    "vocab_list = ['<EOS>'] + fsg.vocabulary_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n",
    "lstm_net = caffe.Net(LSTM_NET_FILE, MODEL_FILE, caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num video features: 130 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 3516, 1, 34, 1101, 1080, 17, 1, 1025, 17, 2, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 3516, 1, 34, 1101, 1080, 17, 1, 1025, 17, 2, 1, 0], 'gt': False, 'prob': [0.98784596, 0.95067114, 0.82986957, 0.43372169, 0.99899989, 0.95077318, 0.9897517, 0.97961217, 0.98964149, 0.99930656, 0.97247314, 0.9998883, 0.99997413, 0.99927467, 0.99992502], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 15, log_p = -1.206992, log_p_word = -0.080466):\n",
      "<en_unk> <en_unk> <en_unk> carefully <en_unk> its historical relationship with <en_unk> starting with the <en_unk>.\n",
      "Num video features: 37 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 45, 1, 2, 1, 1, 12, 1, 0]]\n",
      "[{'caption': [1, 45, 1, 2, 1, 1, 12, 1, 0], 'gt': False, 'prob': [0.99972194, 0.96269906, 0.99856406, 0.99486876, 0.99992383, 0.99959618, 0.99835509, 0.99958581, 0.99946517], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 9, log_p = -0.047950, log_p_word = -0.005328):\n",
      "<en_unk> from <en_unk> the <en_unk> <en_unk> is <en_unk>.\n",
      "Num video features: 49 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 556, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 556, 1, 1, 0], 'gt': False, 'prob': [0.99703157, 0.99434787, 0.97761869, 0.99998748, 0.99992228, 0.99964988], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 6, log_p = -0.031717, log_p_word = -0.005286):\n",
      "<en_unk> <en_unk> hopefully <en_unk> <en_unk>.\n",
      "Num video features: 72 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 657, 2, 19, 3, 114, 5, 75, 82, 4, 116, 74, 17, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 657, 2, 19, 3, 114, 5, 75, 82, 4, 116, 74, 17, 1, 1, 0], 'gt': False, 'prob': [0.99977165, 0.99989033, 0.99969149, 0.94314593, 0.99868208, 0.99643624, 0.99635375, 0.99772662, 0.99947983, 0.99721539, 0.99756396, 0.99928802, 0.99796307, 0.99720395, 0.99954695, 0.99818987, 0.99986207, 0.99998724], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 18, log_p = -0.083713, log_p_word = -0.004651):\n",
      "<en_unk> <en_unk> <en_unk> challenges the deaf and hard of hearing community to come up with <en_unk> <en_unk>.\n",
      "Num video features: 45 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 12, 66, 4703, 13, 145, 102, 10, 2, 1, 1, 0]]\n",
      "[{'caption': [1, 12, 66, 4703, 13, 145, 102, 10, 2, 1, 1, 0], 'gt': False, 'prob': [0.97356206, 0.96849436, 0.995193, 0.97743064, 0.98017794, 0.98406708, 0.99389583, 0.99797934, 0.99762255, 0.99994087, 0.99938726, 0.99977559], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 12, log_p = -0.133958, log_p_word = -0.011163):\n",
      "<en_unk> is very unusual for any university in the <en_unk> <en_unk>.\n",
      "Num video features: 138 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 103, 4, 172, 9, 975, 5, 2, 1, 1, 1, 3, 1, 1, 36, 41, 99, 1, 0]]\n",
      "[{'caption': [1, 103, 4, 172, 9, 975, 5, 2, 1, 1, 1, 3, 1, 1, 36, 41, 99, 1, 0], 'gt': False, 'prob': [0.99711204, 0.69352216, 0.99940825, 0.67602336, 0.95458072, 0.99334478, 0.99879164, 0.99996841, 0.99997413, 0.99750048, 0.99738747, 0.99913424, 0.99774629, 0.99975723, 0.9949227, 0.99773884, 0.99693382, 0.99954945, 0.99995458], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 19, log_p = -0.834816, log_p_word = -0.043938):\n",
      "<en_unk> going to give you examples of the <en_unk> <en_unk> <en_unk> and <en_unk> <en_unk> what do these <en_unk>.\n",
      "Num video features: 118 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 13, 1, 1, 64, 65, 4, 165, 65, 344, 2, 1, 17, 5086, 1, 0]]\n",
      "[{'caption': [1, 13, 1, 1, 64, 65, 4, 165, 65, 344, 2, 1, 17, 5086, 1, 0], 'gt': False, 'prob': [0.85284263, 0.43005177, 0.94522005, 0.99994922, 0.84165114, 0.95941734, 0.9977355, 0.99796009, 0.99396199, 0.96431422, 0.99967349, 0.99455994, 0.99703443, 0.99201125, 0.99996245, 0.99732178], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 16, log_p = -1.339432, log_p_word = -0.083715):\n",
      "<en_unk> for <en_unk> <en_unk> want them to understand them without the <en_unk> with developmental <en_unk>.\n",
      "Num video features: 43 \n",
      "Dimension of video features: (1, 32)\n",
      "[[3, 1, 36, 14, 244, 4, 481, 10, 6, 73, 8, 2869, 1, 5010, 1, 0]]\n",
      "[{'caption': [3, 1, 36, 14, 244, 4, 481, 10, 6, 73, 8, 2869, 1, 5010, 1, 0], 'gt': False, 'prob': [0.99902213, 0.99956161, 0.998447, 0.99002528, 0.99942207, 0.99990737, 0.99421269, 0.9992004, 0.99987793, 0.9970963, 0.99944085, 0.99803084, 0.99995887, 0.99818063, 0.99998116, 0.99975115], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 16, log_p = -0.027961, log_p_word = -0.001748):\n",
      "And <en_unk> what it means to communicate in a language that involves <en_unk> grammatical <en_unk>.\n",
      "Num video features: 71 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 61, 98, 21, 2, 1, 1, 3, 12, 10, 2, 1, 1, 0]]\n",
      "[{'caption': [1, 61, 98, 21, 2, 1, 1, 3, 12, 10, 2, 1, 1, 0], 'gt': False, 'prob': [0.99964094, 0.73271769, 0.95702165, 0.99955004, 0.9999361, 0.99956542, 0.99716187, 0.99856794, 0.99365509, 0.99941027, 0.99924254, 0.9996019, 0.9986344, 0.99998534], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 14, log_p = -0.369999, log_p_word = -0.026429):\n",
      "<en_unk> has been on the <en_unk> <en_unk> and is in the <en_unk> <en_unk>.\n",
      "Num video features: 716 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 788, 40, 2299, 4, 2918, 4, 873, 6, 1, 63, 4, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 788, 40, 2299, 4, 2918, 4, 873, 6, 1, 63, 4, 1, 1, 0], 'gt': False, 'prob': [0.99993718, 0.52252984, 0.98884994, 0.90478295, 0.89967942, 0.7090283, 0.97224909, 0.88976753, 0.89809674, 0.29724336, 0.45714843, 0.26624751, 0.55743027, 0.99186456, 0.74850118, 0.86210841, 0.54898036], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 17, log_p = -6.412017, log_p_word = -0.377177):\n",
      "<en_unk> <en_unk> <en_unk> representatives were unable to commit to vote a <en_unk> time to <en_unk> <en_unk>.\n",
      "Num video features: 29 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 420, 524, 123, 1921, 4182, 0]]\n",
      "[{'caption': [1, 420, 524, 123, 1921, 4182, 0], 'gt': False, 'prob': [0.9992649, 0.9608736, 0.99350363, 0.99409878, 0.99345171, 0.98872304, 0.9929992], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 7, log_p = -0.078020, log_p_word = -0.011146):\n",
      "<en_unk> recently signed two collaboration agreements.\n",
      "Num video features: 59 \n",
      "Dimension of video features: (1, 32)\n",
      "[[310, 24, 245, 993, 17, 239, 1, 1706, 1, 3, 1, 0]]\n",
      "[{'caption': [310, 24, 245, 993, 17, 239, 1, 1706, 1, 3, 1, 0], 'gt': False, 'prob': [0.60880005, 0.98176575, 0.96628076, 0.9863463, 0.99639833, 0.99619895, 0.99996531, 0.99224633, 0.99841988, 0.99911851, 0.9954614, 0.99996519], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 12, log_p = -0.584999, log_p_word = -0.048750):\n",
      "While they may struggle with learning <en_unk> linguistic <en_unk> and <en_unk>.\n",
      "Num video features: 26 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 66, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 66, 1, 0], 'gt': False, 'prob': [0.99976796, 0.97725201, 0.99989271, 0.94002473, 0.99995458, 0.99972206], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 6, log_p = -0.085523, log_p_word = -0.014254):\n",
      "<en_unk> <en_unk> <en_unk> very <en_unk>.\n",
      "Num video features: 45 \n",
      "Dimension of video features: (1, 32)\n",
      "[[572, 2, 2498, 1, 0]]\n",
      "[{'caption': [572, 2, 2498, 1, 0], 'gt': False, 'prob': [0.96439636, 0.99860674, 0.98549646, 0.98686403, 0.99959952], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 5, log_p = -0.065880, log_p_word = -0.013176):\n",
      "Throughout the decades <en_unk>.\n",
      "Num video features: 162 \n",
      "Dimension of video features: (1, 32)\n",
      "[[4451, 1, 1, 926, 39, 6, 681, 1, 4, 260, 1, 1, 0]]\n",
      "[{'caption': [4451, 1, 1, 926, 39, 6, 681, 1, 4, 260, 1, 1, 0], 'gt': False, 'prob': [0.72264707, 0.99973887, 0.96129936, 0.80808425, 0.89324188, 0.90659517, 0.38178322, 0.99739265, 0.36989284, 0.69060063, 0.99268651, 0.9701708, 0.76240969], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 13, log_p = -3.427754, log_p_word = -0.263673):\n",
      "Infant <en_unk> <en_unk> meaning if a professional <en_unk> to show <en_unk> <en_unk>.\n",
      "Num video features: 59 \n",
      "Dimension of video features: (1, 32)\n",
      "[[2, 102, 10, 2062, 2, 298, 1, 0]]\n",
      "[{'caption': [2, 102, 10, 2062, 2, 298, 1, 0], 'gt': False, 'prob': [0.99793977, 0.99682367, 0.99987745, 0.99463993, 0.99874878, 0.99495763, 0.99964595, 0.99991751], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 8, log_p = -0.017485, log_p_word = -0.002186):\n",
      "The university in leading the diversity <en_unk>.\n",
      "Num video features: 75 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 2919, 3, 1706, 1, 8, 11, 251, 388, 40, 568, 1, 0]]\n",
      "[{'caption': [1, 2919, 3, 1706, 1, 8, 11, 251, 388, 40, 568, 1, 0], 'gt': False, 'prob': [0.59813994, 0.96169358, 0.99186641, 0.98906237, 0.99808407, 0.99870074, 0.99481732, 0.99553865, 0.99103397, 0.99577749, 0.99281889, 0.99993587, 0.99998569], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 13, log_p = -0.605563, log_p_word = -0.046582):\n",
      "<en_unk> cognitive and linguistic <en_unk> that we never knew were possible <en_unk>.\n",
      "Num video features: 121 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 2, 1, 1, 1532, 2, 1, 5, 47, 1, 357, 137, 8, 2, 345, 12, 139, 397, 162, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 2, 1, 1, 1532, 2, 1, 5, 47, 1, 357, 137, 8, 2, 345, 12, 139, 397, 162, 1, 0], 'gt': False, 'prob': [0.70459551, 0.98730785, 0.99967527, 0.49535352, 0.99326044, 0.99976832, 0.89711493, 0.99935716, 0.99987781, 0.94776177, 0.99803215, 0.9957782, 0.99565494, 0.99397331, 0.99950504, 0.99981493, 0.99402231, 0.99987686, 0.99038601, 0.99154234, 0.99613369, 0.99596232, 0.9999975], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 23, log_p = -1.285171, log_p_word = -0.055877):\n",
      "<en_unk> <en_unk> <en_unk> the <en_unk> <en_unk> represents the <en_unk> of an <en_unk> making sure that the idea is being put into <en_unk>.\n",
      "Num video features: 97 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 1822, 40, 2299, 4, 2918, 4, 6, 1, 63, 4, 1, 2, 4908, 3, 0]]\n",
      "[{'caption': [1, 1, 1, 1822, 40, 2299, 4, 2918, 4, 6, 1, 63, 4, 1, 2, 4908, 3, 0], 'gt': False, 'prob': [0.987616, 0.46154341, 0.96080351, 0.27913463, 0.44015113, 0.62598044, 0.98086381, 0.99200869, 0.99981338, 0.99106163, 0.99959105, 0.99591798, 0.99995482, 0.98407894, 0.99959713, 0.9965049, 0.99980932, 0.99959046], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 18, log_p = -3.452369, log_p_word = -0.191798):\n",
      "<en_unk> <en_unk> <en_unk> recognizes were unable to commit to a <en_unk> time to <en_unk> the moratorium and.\n",
      "Num video features: 45 \n",
      "Dimension of video features: (1, 32)\n",
      "[[464, 5, 31, 120, 1, 71, 771, 3, 1076, 1, 0]]\n",
      "[{'caption': [464, 5, 31, 120, 1, 71, 771, 3, 1076, 1, 0], 'gt': False, 'prob': [0.98376036, 0.99689794, 0.99452341, 0.98924631, 0.99989343, 0.99715197, 0.99763155, 0.9992674, 0.99714202, 0.9998734, 0.99935907], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 11, log_p = -0.045476, log_p_word = -0.004134):\n",
      "True of all world <en_unk> sign languages and spoken <en_unk>.\n",
      "Num video features: 48 \n",
      "Dimension of video features: (1, 32)\n",
      "[[9, 134, 215, 6, 1, 1, 1, 575, 10, 2, 1, 3, 660, 1, 0]]\n",
      "[{'caption': [9, 134, 215, 6, 1, 1, 1, 575, 10, 2, 1, 3, 660, 1, 0], 'gt': False, 'prob': [0.99924636, 0.99605042, 0.99610424, 0.99907589, 0.99946088, 0.99957293, 0.9997806, 0.99586284, 0.99882072, 0.99982029, 0.99978489, 0.99632806, 0.99803907, 0.99933428, 0.99976045], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 15, log_p = -0.022993, log_p_word = -0.001533):\n",
      "You could become a <en_unk> <en_unk> <en_unk> interested in the <en_unk> and study <en_unk>.\n",
      "Num video features: 67 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 460, 33, 23, 3925, 4, 170, 13, 1, 0]]\n",
      "[{'caption': [1, 460, 33, 23, 3925, 4, 170, 13, 1, 0], 'gt': False, 'prob': [0.99334478, 0.97485197, 0.99924731, 0.99525177, 0.99286848, 0.99989617, 0.99672687, 0.99925023, 0.99987018, 0.99999583], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 10, log_p = -0.049083, log_p_word = -0.004908):\n",
      "<en_unk> alumni will be returning to campus for <en_unk>.\n",
      "Num video features: 50 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 1, 1, 1, 1, 0], 'gt': False, 'prob': [0.6062516, 0.88055158, 0.81986558, 0.91202611, 0.84519154, 0.87185585, 0.50492615, 0.76358956], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 8, log_p = -2.176760, log_p_word = -0.272095):\n",
      "<en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 89 \n",
      "Dimension of video features: (1, 32)\n",
      "[[10, 1, 1, 1, 3, 2, 1, 0]]\n",
      "[{'caption': [10, 1, 1, 1, 3, 2, 1, 0], 'gt': False, 'prob': [0.89933783, 0.99531615, 0.99394774, 0.99723583, 0.99906546, 0.99988091, 0.99928111, 0.99977797], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 8, log_p = -0.121625, log_p_word = -0.015203):\n",
      "In <en_unk> <en_unk> <en_unk> and the <en_unk>.\n",
      "Num video features: 55 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 133, 13, 1, 1, 1, 3, 1, 276, 1, 0]]\n",
      "[{'caption': [1, 133, 13, 1, 1, 1, 3, 1, 276, 1, 0], 'gt': False, 'prob': [0.80059433, 0.99552119, 0.99862969, 0.9994815, 0.99972147, 0.99996662, 0.97563863, 0.9995389, 0.99301845, 0.99974424, 0.99981076], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 11, log_p = -0.261667, log_p_word = -0.023788):\n",
      "<en_unk> look for <en_unk> <en_unk> <en_unk> and <en_unk> find <en_unk>.\n",
      "Num video features: 17 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 566, 123, 1, 785, 0]]\n",
      "[{'caption': [1, 566, 123, 1, 785, 0], 'gt': False, 'prob': [0.99985075, 0.98461181, 0.97811514, 0.99930215, 0.99412125, 0.9997856], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 6, log_p = -0.044594, log_p_word = -0.007432):\n",
      "<en_unk> teach two <en_unk> classes.\n",
      "Num video features: 49 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 1, 0], 'gt': False, 'prob': [0.996548, 0.97977614, 0.99930334, 0.99696511, 0.95252621], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 5, log_p = -0.076263, log_p_word = -0.015253):\n",
      "<en_unk> <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 39 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 228, 32, 228, 13, 19, 3, 114, 5, 75, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 228, 32, 228, 13, 19, 3, 114, 5, 75, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'gt': False, 'prob': [0.99958831, 0.9304083, 0.34807828, 0.42519322, 0.43246165, 0.15247072, 0.99964976, 0.85316062, 0.86724567, 0.91244197, 0.22492392, 0.77041316, 0.75894016, 0.77605611, 0.25525889, 0.66902363, 0.70928323, 0.66654819, 0.52744257, 0.66115659], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 20, log_p = -10.947569, log_p_word = -0.547378):\n",
      "<en_unk> research about research for deaf and hard of hearing and <en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 36 \n",
      "Dimension of video features: (1, 32)\n",
      "[[16, 9, 38, 116, 3, 821, 52, 1, 0]]\n",
      "[{'caption': [16, 9, 38, 116, 3, 821, 52, 1, 0], 'gt': False, 'prob': [0.98875195, 0.99960357, 0.99597001, 0.99799889, 0.99988878, 0.99775583, 0.9983955, 0.99999952, 0.99997306], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 9, log_p = -0.021741, log_p_word = -0.002416):\n",
      "So you can come and visit me <en_unk>.\n",
      "Num video features: 85 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 77, 688, 6, 82, 1258, 1034, 364, 35, 2, 1, 1, 0]]\n",
      "[{'caption': [1, 77, 688, 6, 82, 1258, 1034, 364, 35, 2, 1, 1, 0], 'gt': False, 'prob': [0.50081456, 0.80121261, 0.52435064, 0.96846324, 0.97486967, 0.98765951, 0.98343992, 0.98549598, 0.99591148, 0.99782628, 0.9989748, 0.99800044, 0.99924386], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 13, log_p = -1.670022, log_p_word = -0.128463):\n",
      "<en_unk> also held a community town hall meeting at the <en_unk> <en_unk>.\n",
      "Num video features: 58 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1008, 6, 511, 1176, 1242, 30, 773, 13, 372, 221, 0]]\n",
      "[{'caption': [1, 1, 1008, 6, 511, 1176, 1242, 30, 773, 13, 372, 221, 0], 'gt': False, 'prob': [0.95835078, 0.99934632, 0.97140098, 0.99883026, 0.9735536, 0.98641413, 0.9928509, 0.99564821, 0.99223268, 0.99850512, 0.99566615, 0.9967618, 0.99997342], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 13, log_p = -0.142306, log_p_word = -0.010947):\n",
      "<en_unk> <en_unk> wrote a position statement regarding our goals for relay services.\n",
      "Num video features: 49 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 1, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 1, 1, 1, 1, 0], 'gt': False, 'prob': [0.48636356, 0.82993668, 0.85203499, 0.95810372, 0.83297461, 0.83961016, 0.62993103, 0.55249286], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 8, log_p = -2.523161, log_p_word = -0.315395):\n",
      "<en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 73 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 1, 1, 1, 0], 'gt': False, 'prob': [0.99995339, 0.9084546, 0.72563744, 0.99921441, 0.99337703], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 5, log_p = -0.424193, log_p_word = -0.084839):\n",
      "<en_unk> <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 54 \n",
      "Dimension of video features: (1, 32)\n",
      "[[3, 61, 251, 828, 6, 891, 73, 8, 1385, 1, 819, 1, 37, 1987, 1, 0]]\n",
      "[{'caption': [3, 61, 251, 828, 6, 891, 73, 8, 1385, 1, 819, 1, 37, 1987, 1, 0], 'gt': False, 'prob': [0.99958736, 0.99601835, 0.99633908, 0.99100608, 0.99950802, 0.99330783, 0.99750936, 0.99805582, 0.99356163, 0.99935275, 0.99718839, 0.99975318, 0.99653167, 0.99875975, 0.99999154, 1.0], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 16, log_p = -0.043644, log_p_word = -0.002728):\n",
      "And has never experienced a visual language that uses <en_unk> body <en_unk> or facial <en_unk>.\n",
      "Num video features: 40 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 22, 333, 35, 1749, 13, 6, 1, 1, 491, 1, 0]]\n",
      "[{'caption': [1, 22, 333, 35, 1749, 13, 6, 1, 1, 491, 1, 0], 'gt': False, 'prob': [0.99999154, 0.98279321, 0.98089486, 0.99429321, 0.99173236, 0.9980914, 0.99956363, 0.99676836, 0.99942797, 0.99689829, 0.99934572, 0.99879885], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 12, log_p = -0.061799, log_p_word = -0.005150):\n",
      "<en_unk> are looking at strategies for a <en_unk> <en_unk> across <en_unk>.\n",
      "Num video features: 49 \n",
      "Dimension of video features: (1, 32)\n",
      "[[3650, 779, 779, 1, 0]]\n",
      "[{'caption': [3650, 779, 779, 1, 0], 'gt': False, 'prob': [0.36035827, 0.94806373, 0.98727906, 0.99976081, 0.99988449], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 5, log_p = -1.087147, log_p_word = -0.217429):\n",
      "Ohhh run run <en_unk>.\n",
      "Num video features: 49 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0]]\n",
      "[{'caption': [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0], 'gt': False, 'prob': [0.67396057, 0.68857419, 0.99994922, 0.37371165, 0.99999893, 0.89564782, 0.99999928, 0.96774852, 0.99999881, 0.95101184, 0.99999845, 0.92693871, 0.99999774, 0.89247507, 0.99999642, 0.78926188, 0.99999118, 0.61934197, 0.99998009, 0.49175438, 0.99996448, 0.52619958, 0.99822861, 0.67249233], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 24, log_p = -4.601099, log_p_word = -0.191712):\n",
      "<en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> the <en_unk> <en_unk> <en_unk>.\n",
      "Num video features: 70 \n",
      "Dimension of video features: (1, 32)\n",
      "[[891, 3944, 38, 3113, 4, 31, 891, 1, 3, 4, 31, 1, 0]]\n",
      "[{'caption': [891, 3944, 38, 3113, 4, 31, 891, 1, 3, 4, 31, 1, 0], 'gt': False, 'prob': [0.99475396, 0.98809952, 0.99634832, 0.98333299, 0.99758828, 0.9948526, 0.99688065, 0.99945444, 0.99670571, 0.99985909, 0.99952531, 0.99988472, 0.99999917], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 13, log_p = -0.052974, log_p_word = -0.004075):\n",
      "Visual processing can afford to all visual <en_unk> and to all <en_unk>.\n",
      "Num video features: 56 \n",
      "Dimension of video features: (1, 32)\n",
      "[[25, 6, 1043, 45, 1, 0]]\n",
      "[{'caption': [25, 6, 1043, 45, 1, 0], 'gt': False, 'prob': [0.83214688, 0.9886989, 0.98265439, 0.98875952, 0.99995792, 0.99889022], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 6, log_p = -0.225066, log_p_word = -0.037511):\n",
      "As a representative from <en_unk>.\n",
      "Num video features: 14 \n",
      "Dimension of video features: (1, 32)\n",
      "[[1, 201, 5861, 4, 138, 2, 1, 1, 0]]\n",
      "[{'caption': [1, 201, 5861, 4, 138, 2, 1, 1, 0], 'gt': False, 'prob': [0.97830266, 0.98996514, 0.99914813, 0.99753153, 0.9988656, 0.99992299, 0.99971038, 0.99990153, 0.99996364], 'source': {'beam_size': 1, 'type': 'beam'}}]\n",
      "Generated caption (length 9, log_p = -0.036982, log_p_word = -0.004109):\n",
      "<en_unk> after deciding to take the <en_unk> <en_unk>.\n"
     ]
    }
   ],
   "source": [
    "for c in range(START_CHUNK, NUM_CHUNKS):\n",
    "    chunk_start = c * NUM_OUT_PER_CHUNK\n",
    "    chunk_end = (c + 1) * NUM_OUT_PER_CHUNK\n",
    "    chunk = video_gt_pairs.keys()[chunk_start:chunk_end]\n",
    "    html_out_filename = '%s/%s.%s.%d_to_%d.html' % \\\n",
    "      (RESULTS_DIR, data_split_name, NET_TAG, chunk_start, chunk_end)\n",
    "    text_out_filename = '%s/%s.%s_' % \\\n",
    "      (RESULTS_DIR, data_split_name, NET_TAG)\n",
    "\n",
    "    if not os.path.exists(RESULTS_DIR): os.makedirs(RESULTS_DIR)\n",
    "    outputs = run_pred_iters(lstm_net, chunk, video_gt_pairs,\n",
    "                fsg, strategies=STRATEGIES, display_vocab=vocab_list)\n",
    "\n",
    "    html_out = to_html_output(outputs, vocab_list)\n",
    "    html_out_file = open(html_out_filename, 'w')\n",
    "    html_out_file.write(html_out)\n",
    "    html_out_file.close() \n",
    "    # break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'--u_-w9WCG8_14'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ce28387ec3ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvocab_inds_to_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'--u_-w9WCG8_14'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'caption'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '--u_-w9WCG8_14'"
     ]
    }
   ],
   "source": [
    "vocab_inds_to_sentence(vocab_list, outputs['--u_-w9WCG8_14'][0]['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 4,\n",
       " 563,\n",
       " 113,\n",
       " 275,\n",
       " 558,\n",
       " 360,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for it, frame in enumerate(x):\n",
    "    if it == 0:\n",
    "        ifSeq = 0\n",
    "    else:\n",
    "        ifSeq = 1\n",
    "    lstm_net.forward(frames_fc7=frame, cont_sentence=np.array([ifSeq]), input_sentence=np.array([0]), stage_indicator=np.array([0]))  # 0 is EOS, 1 is unique token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = video_to_descriptor('FvWTyzjD690_235', fsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[x > 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_net.blobs['cont_sentence'].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " 15.0,\n",
       " 2.0,\n",
       " 151.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -1.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FvWTyzjD690_235'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ac87a330cd1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvideo_gt_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FvWTyzjD690_235'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'FvWTyzjD690_235'"
     ]
    }
   ],
   "source": [
    "video_gt_pairs['FvWTyzjD690_235'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "data = h5py.File('/data/gengshan/pose_s2vt/hdf5/buffer_32_s2vt_100/test_batches/batch_0.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['cont_sentence'][:100, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 have the same <en_unk> <EOS> 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01...'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inds_to_sentence(vocab_list, [int(x) for x in data['target_sentence'][:100,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = lstm_net.forward(frames_fc7=data['frame_fc7'][:100, :1, :], \\\n",
    "                 cont_sentence=data['cont_sentence'][:100, :1],\\\n",
    "                 input_sentence=data['input_sentence'][:100, :1],\\\n",
    "                 stage_indicator=data['stage_indicator'][:100, :1])['probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1, 6068)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<caffe._caffe.Net at 0x7faeb7b92998>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
