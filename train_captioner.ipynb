{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataLoader import fc7FrameSequenceGenerator\n",
    "from hdf5_npstreamsequence_generator import HDF5SequenceWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start every sentence in a new array, pad if <max\n",
    "MAX_WORDS = 160 # 80  # max length of whole sequence\n",
    "# BUFFER_SIZE = 1 # TEXT streams\n",
    "BUFFER_SIZE = 32 # number of streams for a batch of data\n",
    "\n",
    "BATCH_STREAM_LENGTH = 1000  # length for a stream of data \n",
    "SETTING = '/data/gengshan/pose_s2vt'\n",
    "FRAMEFEAT_FILE_PATTERN = SETTING + '/splits/dataCsv_{0}.txt'\n",
    "SENTS_FILE_PATTERN = SETTING + '/splits/dataTsv_{0}.txt'  # input paths\n",
    "\n",
    "OUTPUT_DIR = '{0}/hdf5/buffer_{1}_s2vt_{2}'.format(SETTING, BUFFER_SIZE, MAX_WORDS)\n",
    "OUTPUT_DIR_PATTERN = '%s/%%s_batches' % OUTPUT_DIR\n",
    "VOCAB = '%s/vocabulary.txt' % SETTING\n",
    "\n",
    "OUT_FILE_PATTERN = SETTING + '/rawcorpus/{0}/s2vt_vgg_{0}_sequence.txt'\n",
    "OUT_CORPUS_PATH = SETTING + '/rawcorpus/{0}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(split_name, data_split_name, batch_stream_length,\n",
    "                      aligned=False, reverse=False):\n",
    "    filenames = [FRAMEFEAT_FILE_PATTERN.format(data_split_name),\n",
    "               SENTS_FILE_PATTERN.format(data_split_name)]\n",
    "    vocab_filename = '/data/gengshan/s2vt/vocabulary_whole.txt'  # VOCAB\n",
    "    output_path = OUTPUT_DIR_PATTERN % split_name\n",
    "    aligned = True\n",
    "    \n",
    "    \n",
    "    fsg = fc7FrameSequenceGenerator(filenames, BUFFER_SIZE, vocab_filename,\n",
    "         max_words=MAX_WORDS, align=aligned, shuffle=True, pad=aligned,\n",
    "         truncate=aligned, reverse=reverse)\n",
    "   \n",
    "    \n",
    "    fsg.batch_stream_length = batch_stream_length\n",
    "    writer = HDF5SequenceWriter(fsg, output_dir=output_path)\n",
    "    \n",
    "    writer.write_to_exhaustion()\n",
    "\n",
    "    writer.write_filelists()  # h5 file list\n",
    "    if not os.path.isfile(vocab_filename):\n",
    "        print \"Vocabulary not found\"\n",
    "        # fsg.dump_vocabulary(vocab_out_path)\n",
    "    out_path = OUT_CORPUS_PATH.format(data_split_name)\n",
    "    vid_id_order_outpath = '%s/yt_s2vtvgg_%s_vidid_order_%d_%d.txt' % \\\n",
    "    (out_path, data_split_name, BUFFER_SIZE, MAX_WORDS)\n",
    "    frame_sequence_outpath = '%s/yt_s2vtvgg_%s_sequence_%d_%d_recurrent.txt' % \\\n",
    "    (out_path, data_split_name, BUFFER_SIZE, MAX_WORDS)\n",
    "    print(vid_id_order_outpath)\n",
    "    print(frame_sequence_outpath)\n",
    "    fsg.dump_video_file(vid_id_order_outpath, frame_sequence_outpath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading frame features from file: /data/gengshan/pose_s2vt/splits/dataCsv_haha.txt\n",
      "Reading sentences in: /data/gengshan/pose_s2vt/splits/dataTsv_haha.txt\n",
      "Initializing the vocabulary.\n",
      "Initialized vocabulary from file with 6067 unique words (from 6066 total words in dataset).\n",
      "Processed 0/608 (0.000000%) lines\n",
      "Processed 1/608 (0.164474%) lines\n",
      "Processed 10/608 (1.644737%) lines\n",
      "Processed 20/608 (3.289474%) lines\n",
      "Processed 30/608 (4.934211%) lines\n",
      "Processed 40/608 (6.578947%) lines\n",
      "Processed 50/608 (8.223684%) lines\n",
      "Processed 60/608 (9.868421%) lines\n",
      "Processed 70/608 (11.513158%) lines\n",
      "Processed 80/608 (13.157895%) lines\n",
      "Processed 90/608 (14.802632%) lines\n",
      "Processed 100/608 (16.447368%) lines\n",
      "Processed 110/608 (18.092105%) lines\n",
      "Processed 120/608 (19.736842%) lines\n",
      "Processed 130/608 (21.381579%) lines\n",
      "Processed 140/608 (23.026316%) lines\n",
      "Processed 150/608 (24.671053%) lines\n",
      "Processed 160/608 (26.315789%) lines\n",
      "Processed 170/608 (27.960526%) lines\n",
      "Processed 180/608 (29.605263%) lines\n",
      "Processed 190/608 (31.250000%) lines\n",
      "Processed 200/608 (32.894737%) lines\n",
      "Processed 210/608 (34.539474%) lines\n",
      "Processed 220/608 (36.184211%) lines\n",
      "Processed 230/608 (37.828947%) lines\n",
      "Processed 240/608 (39.473684%) lines\n",
      "Processed 250/608 (41.118421%) lines\n",
      "Processed 260/608 (42.763158%) lines\n",
      "Processed 270/608 (44.407895%) lines\n",
      "Processed 280/608 (46.052632%) lines\n",
      "Processed 290/608 (47.697368%) lines\n",
      "Processed 300/608 (49.342105%) lines\n",
      "Processed 310/608 (50.986842%) lines\n",
      "Processed 320/608 (52.631579%) lines\n",
      "Processed 330/608 (54.276316%) lines\n",
      "Processed 340/608 (55.921053%) lines\n",
      "Processed 350/608 (57.565789%) lines\n",
      "Processed 360/608 (59.210526%) lines\n",
      "Processed 370/608 (60.855263%) lines\n",
      "Processed 380/608 (62.500000%) lines\n",
      "Processed 390/608 (64.144737%) lines\n",
      "Processed 400/608 (65.789474%) lines\n",
      "Processed 410/608 (67.434211%) lines\n",
      "Processed 420/608 (69.078947%) lines\n",
      "Processed 430/608 (70.723684%) lines\n",
      "Processed 440/608 (72.368421%) lines\n",
      "Processed 450/608 (74.013158%) lines\n",
      "Processed 460/608 (75.657895%) lines\n",
      "Processed 470/608 (77.302632%) lines\n",
      "Processed 480/608 (78.947368%) lines\n",
      "Processed 490/608 (80.592105%) lines\n",
      "Processed 500/608 (82.236842%) lines\n",
      "Processed 510/608 (83.881579%) lines\n",
      "Processed 520/608 (85.526316%) lines\n",
      "Processed 530/608 (87.171053%) lines\n",
      "Processed 540/608 (88.815789%) lines\n",
      "Processed 550/608 (90.460526%) lines\n",
      "Processed 560/608 (92.105263%) lines\n",
      "Processed 570/608 (93.750000%) lines\n",
      "Processed 580/608 (95.394737%) lines\n",
      "Processed 590/608 (97.039474%) lines\n",
      "Processed 600/608 (98.684211%) lines\n",
      "/data/gengshan/pose_s2vt/hdf5/buffer_32_s2vt_160/HAHAHAHA_batches/hdf5_chunk_list.txt\n",
      "/data/gengshan/pose_s2vt/rawcorpus/haha/yt_s2vtvgg_haha_vidid_order_32_160.txt\n",
      "/data/gengshan/pose_s2vt/rawcorpus/haha/yt_s2vtvgg_haha_sequence_32_160_recurrent.txt\n",
      "Dumping vidid order to file: /data/gengshan/pose_s2vt/rawcorpus/haha/yt_s2vtvgg_haha_vidid_order_32_160.txt\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# split_name, data_split_name, aligned \n",
    "DATASETS = [('HAHAHAHA', 'haha', False, False)]\n",
    "for split_name, data_split_name, aligned, reverse in DATASETS:\n",
    "    preprocess_dataset(split_name, data_split_name, BATCH_STREAM_LENGTH,aligned, reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
